{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddb3a62-e801-4f3f-9744-d8ddbe027ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Games\\123\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95dcddf7-82c7-4687-ba73-c5ce7eec30bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 1.52G/1.52G [02:24<00:00, 10.5MB/s]\n",
      "D:\\Games\\123\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\seoul\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 1.21kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf21369-e70e-4db2-af93-49b6d4aecc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13676b1d-4f7a-486c-b7ff-ee58cba1408c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_some_text(input_str, text_len = 250):\n",
    "\n",
    "    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(text_len):\n",
    "            outputs = model(cur_ids, labels=cur_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
    "\n",
    "        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62028a83-ee7d-4f16-b8ee-e7688ea07cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want  to  paint  it  black  because  I  cant afford it.\n",
      "\n",
      "I want to paint  me  black  because  I  cant afford it.\"\n",
      "\n",
      "In a separate case, the judge said in his ruling that the defendant had been given an alternative punishment, but it was too late by the time they got there. The judge said the victim and defendant had been given the same sentence as if their cases had been transferred.\n",
      "\n",
      "\"The victim, a woman who was raped by a man and her husband, is still suffering from her injuries,\" he said. \"I believe the judge should have sentenced her for the crime of rape. He did not.\"<|endoftext|>When people talk about the importance of the \"fiscal cliff,\" they usually mean the federal deficit. The fiscal cliff is the last major federal tax increase before the debt ceiling is hit in March.\n",
      "\n",
      "The federal government is currently spending $1.3 trillion less than it takes in, or about 1.5 percent of the GDP, and that's with a $85 trillion debt ceiling that is set to go into effect on March 15.\n",
      "\n",
      "But when Congress passed their spending bill in December, many Republicans were hoping that they could get to zero in time to avoid a debt-ce\n"
     ]
    }
   ],
   "source": [
    "generate_some_text(\"I want  to  paint  it  black \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479df207-1f4e-45b4-afb8-920a1f089dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
